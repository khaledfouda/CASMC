---
title: "7 - Soft Impute (ALS) with Covariates and Sparse + Low-Rank"
author: "Khaled Fouda"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---


```{r setup, include=TRUE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, eval = FALSE)
library(kableExtra)
library(magrittr)
library(tidyverse)
library(softImpute)

knitr::opts_knit$set(root.dir="/mnt/campus/math/research/kfouda/main/HEC/Youssef/HEC_MAO_COOP/")
```


```{r include=TRUE, warning=FALSE, message=FALSE, eval=TRUE}
source("./code_files/import_lib.R")
```

# References

[1] Hastie (2015) Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares.

[2] [SoftImpute Github Implementation](https://github.com/cran/softImpute/tree/b497771beea6c466266082524a9b0c91b98126a7)


# Notation:

[1] When a matrix is called Sparse below that means it has a sparse class from the package Matrix in R which saves the matrix as 3 lists: list of non-missing values, list of row locations and a list of column locations.

Building on the model in 3 - Alternating Least Squares, we have defined and implemented the ALS method with covariates where we compute the full $\hat{M}=S_\lambda(Y)=UD_\lambda V'$ at each iteration and where we have $\hat{Y}$ the filled response matrix where the missing values are filled with predictions from $\hat{M}$. 

Here, we attempt to implement the method by separting $\hat{Y}$ into a Sparse matrix  (misssing values are kept as 0) and a low rank matrix (containing predictions). Moreover, We attempt to avoid computing $\hat{M}=S_\lambda(Y)=UD_\lambda V'$ at each step and we rely on the following quantities for the updates $U, UD,D,DV,V$. This avoid matrix operations on matrices of dimensions $nxm$. 


We begin by illustrating the Authors' approach for the case where there are no covariates, then, we will adapt the formulas for the covariates. 


Recall our objective function

$$
\begin{aligned}
minimize && \frac{1}{2} \|Y-AB^T - X\beta\|^2_F + \frac{\lambda}{2} (||A||^2_F+||B||^2_F)  && (ALS1)
\end{aligned}
$$


Where $Y \in \Re^{n\times m}$, $A \in \Re^{n\times r}$, $B \in \Re^{m \times r},$ and $X \in \Re^{n\times k}$.
We assume that $max(r,k) <<< min(m,n)$. 

Recall Algorithm ALS/Covarites in our document (3-ALS.html), at step t,  we begin by fixing $A$ and we solve for $B$ and $\beta$ as:
  
   -  $DB^T = (D^2 + \lambda I)^{-1}D^2U^TY^+ = (D^2/(D^2+\lambda))U^TY^+$
   -  $\bar{B}D = \tilde{U}\tilde{D}^2\tilde{V}^T$
      -  $V \leftarrow \tilde{U}$
      -  $D^2 \leftarrow \tilde{D^2}$
   - $Y^*[\overline\Omega] = AB^T[\overline{\Omega}] + X \hat\beta[\overline{\Omega}] = UD^2V^T[\overline \Omega] + X \hat \beta[\overline{\Omega}]$
   - $\hat{\beta} = (X^TX)^{-1} X^T Y^*$
   -  $Y^+= Y^* - X \hat \beta$
   
   
Our objective, is to avoid any matrix operation that will result in a $n\times m$ dimensions. That is, we want to replace $UD^2V^T$, $X\hat\beta$, $Y+$, and $Y^*$ with either lower rank matrices, sparse matrices, or a combination of both.

We begin by defining the Hat (projection) matrix of X since this matrix will be fixed for the model fitting and will only be computed once. Note that this matrix have a high dimension, and high rank, which are two things we want to avoid. 

$$
H = X (X^TX)^{-1} X^T
$$
Let 

$$
H^* = (I_n-H).
$$

However, to reduce computational time, we estimated this matrix using the QR decomposition,
let Q be the Q matrix in the QR decomposition, then

$$
 \hat{H} = Q Q^T
$$

Moreover, we define the following two low rank matrices which will be very useful in our calculations

$$
\begin{aligned}
V^* = V D^2  & \in \Re^{m \in r}\\
U^* = U D^2  & \in \Re^{n \in r}\\
\end{aligned}
$$
We let $Y^s$ be Sparse Matrix where the cells corresponding to the missing values are set to 0. Similarly, any matrix and superscript $^s$ is a sparse version of said matrix where the cells corresponding to the missing values in $Y$ are set to 0. For example, we have $H^s$ which is a sparse version of $H$ where we only keep the values ....


in our algorithm, we assume that $Y^+$ is the version of $Y$ where the missing values are replace by estimates from $\hat{M}$ and that the covariates effect is removed from all cells. 
It's worth noting that $\hat{M}$ are the predictions of $Y$ *after* removing the covariates effect. That is, if we want a version of $Y$ where the missing values are replaced by predictions but without adjusting for the covariates, we need to add $X\hat\beta$ back. Let $Y^*$ be this matrix.

Let $C = X^T\beta=HY^*$ be the covariates effects, and $C'$ be the estimates from the previous iteration.
then we have: 


$$
\begin{aligned}
Y^* &= P_{\Omega}(Y - \hat{M}) - P_{\Omega}( C') + \hat{M} + C' &&\\&= S  - P_{\Omega}( C') + \hat{M} + C' && (\text{SPLR 1}) \\
Y^+ &= Y^* - C &&\\
& = Y^* - HY^* \\
&= (I_n-H) Y^*
\end{aligned}
$$
Where $S = P_{\Omega}(Y - \hat{M})$ is a sparse matrix, $P_{\Omega}( C')$ is also a sparse matrix of the covariate estimates for the observed data, and $\hat{M}$ is a product of low-rank matrices. Our bottleneck here is the matrix $C$ of dimension $n \times m$.

We will explore each of the four quantities in equation (SPLR 1) and how to optimize their calculations, but first let's look at the estimates of $A$ and $B$.


From our previous document We have $\hat{B}$ represented as (note that the formula below estimates DB instead of B since we take the SVD of BD and not B)



$$
\hat{B}^T = D^* U^T Y^+,
$$

where $D^* = D^2/(D^2+\lambda)$ being a vector of length $k$. 
Let $U_H = U^T(I_n-H)$ be the product of the low rank matrix $U$ and the projection matrix. Although this product is expensive, it results a low-rank matrix and is used in multiple places.

Using our new expansion of $Y^+$ we get


$$
\begin{aligned}
\hat{B}^T &= D^* (U^TH^*S - U^TH^*P_{\Omega}( C') + U^TH^* U DV^T + U^TH^*C')\\
& = D^* (U_H S - U_HP_{\Omega}( C') + U_H U DV^T + U_HC')
\end{aligned}
$$
Let's consider each part of them individually:

1. $U_H S$ is a sparse matrix multiplied by a low rank matrix. This is done efficiently by R.
2. $U_H U DV^T$ The only escape from this is to carefully choose the order of the multiplication so that the dimensions of each product is always the lowest. The order is $(((U_H U) D)V^T)$
3. $U_H C'$: Assuming no regularization on the covariates, the Hat matrix is then idempotent.

$$
\begin{aligned}
U_H C' &= U^T(I_n-H)HY^*_{old} \\
& = U^T(H-H.H) Y^*_{old} \\
& = U^T(H-H) Y^*_{old} \\
& = 0
\end{aligned}
$$
4. $U_H P_{\Omega}( C')$:

At step $i$ we have

$$
\begin{aligned}
P_{\Omega}( C_{(i-1)}) &= P_{\Omega}( HY_{i-1}^*)\\
&= P_{\Omega}(HY - H\hat{M}_{(i-1)} + HH C_{(i-2)})\\
&= P_{\Omega}(HY - H\hat{M}_{(i-1)} + H C_{(i-2)}) && \text{By idempotent property} \\
&= P_{\Omega}(HY - H\hat{M}_{(i-1)} - HY + H\hat{M}_{(i-2)} + H.H C_{(i-3)}) \\
& = \cdots\\
& = P_{\Omega}(H (-\hat{M}_{(i-1)}+\hat{M}_{(i-2)}-\hat{M}_{(i-3)}+\cdots))
\end{aligned}
$$



---------------------------------------------

Let's expand $C$:

$$
\begin{aligned}
C &= HY^* = HS + H\hat{M} + HC'\\ &= HS + HU{V^*}^T + HC' \\
C' &= HS + HU{V^*}^T + HC''
\end{aligned}
$$

An interesting 






