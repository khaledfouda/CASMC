---
title: "8 - Matrix Completition When the True Rank is Known in Advance"
author: "Khaled Fouda"
output:
  html_document:
    df_print: paged
    output_file: ../html/8_Sparse_Zero_Intercept_Rank_Constrained_Imputation.html
editor_options:
  chunk_output_type: console
---



```{r setup, include=TRUE, warning=FALSE, message=FALSE}
library(kableExtra)
library(magrittr)
library(tidyverse)
library(softImpute)
root_dir = "/mnt/campus/math/research/kfouda/main/HEC/Youssef/HEC_MAO_COOP/"
knitr::opts_knit$set(root.dir=root_dir)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, eval = FALSE, 
                      cache.path = paste0(root_dir,"/Documentation/cache/"))
```



```{r include=TRUE, warning=FALSE, message=FALSE, eval=TRUE}
source("./code_files/import_lib.R")
```

# References



# Notation Remark

We will fix the notation as set in the Overleaf summary document. 



# Motivation

After Estimating the Low-rank matrix $M$ and obtaining a good estimates for $P_\Omega(X\beta)$, we are now interesting in obtaining the rest of the matrix (i.e., $P_{\overline\Omega}(X\beta)$).

We will treat $X \beta$ is a partially observed matrix with rank $k$ ($k$ is at most, equal to the number of covariates). Since the rank is know in advance, we no longer need the nuclear norm and our model should be more simple and faster to converge.

We will consider two models. The first model attempts to directly complete $X \beta$ based only on the observed part. The second model will separate $X$ and $\beta$, make use of the covariate matrix $X$, and will attempt to predict $\beta$.

# Problem Set-up

In the context of our model, we have

$$
Y = Y - P_{\Omega}(\hat{M}).
$$
Which lays the ground to solving the following problem:

$$
Y = X \beta = Z
$$
### Note: The first model here is just for illustration but we decided to move forward with the second model as it provided better performance.


For the first Model, assume that we have a partially observed matrix $Y$ of rank $k$. We want to predict a low-rank matrix $Z$ such that 

$$
\| P_\Omega(Y)-P_\Omega(Z) \|^2_F < \epsilon,
$$

and

$$
\| Z\|_0 = \sum_i I(\sigma_i>0) = k.
$$

We will consider the Alternating-Least-Squares model and therefore $Z=AB^T=UD^2V^T$ where $A=UD$ and $B=VD$.

The loss function is similar to the original soft-impute but without the regularization.

$$
\text{minimize     } \| Y-AB^T\|^2_F\\ \text{      subject to } \|A\|_0=\|B\|_0=k
$$

Since the $L_0$ is non-convex we cannot use it in the loss function. However, the constraint can be easily applied while projecting $A$ into its svd decomposition.  Generally, if $A$ and $B$ were initialized with rank $k$, no extra steps are required since the rank won't changed (unless we want to trim).

We still consider the sparse + low-rank representation of $Y^*$ (observed are untouched and missing are filled with predictions) as:

$$
\begin{aligned}
Y^* &= P_\Omega(Y-AB^T) + AB^T \\
&= Y - SPROD(U,D^2V^T) + UD^2V^T \\
& = S+ UD^2V^T.
\end{aligned}
$$

The least squares update functions are:

$$
\begin{aligned}
BD = (Y^*)^TU  = S^TU + VD^2\\
AD = Y^* V = SV + UD^2
\end{aligned}
$$

We compute the svd decomposition of $BD$ and $AD$ at every step and update $U$, $V$, $D$, and $S$ accordingly.


---------------------------------------

# Naive MC (For Initialization)

Replacing the random initialization of $U$, $D$, and $V$ by the decomposition of a naive matrix completition cuts the number of iterations needed for convergence by half. For a partially observed matrix Y, the naive MC is defined as:

$$
\begin{aligned}
&P_\Omega(Y_{naive}) = P_\Omega(Y),\\
&P_{\overline\Omega}(Y_{naive})[i,j] = (\overline{Y[i,]}+\overline{Y[,j]})/2.
\end{aligned}
$$

In other words, the missing values are filled with the average of the mean row and column values (considering only observed cells).

The code for the Naive MC can be found under /functions/utils/naive_MC.R. The function expects a partially observed matrix where the missing values are filled with NAs. It returns a filled matrix.
The code for the model above can be found under /functions/SoftImpute_Incomplete/fit_no_covariates_fixed_rank.R.


<!-- We now illustrate our model above: -->


```{r eval=FALSE, echo=FALSE}

# first we generate simulation data and estimating the low-rank M and the observed Xbeta
setwd("/mnt/campus/math/research/kfouda/main/HEC/Youssef/HEC_MAO_COOP")
source("./code_files/import_lib.R")
gen.dat <- generate_simulation_data_ysf(2,800,700,10,10, missing_prob = 0.9,coll=T)
W_valid <- matrix.split.train.test(gen.dat$W, testp=0.2)
Y_train = (gen.dat$Y * W_valid)
Y_valid = gen.dat$Y[W_valid==0]
y = yfill = Y_train
y[y==0] = NA
ys <- as(y, "Incomplete")
X_r = reduced_hat_decomp(gen.dat$X, 1e-2)
beta_partial = MASS::ginv(t(X_r$X) %*% X_r$X) %*% t(X_r$X)
fit1 = simpute.als.fit_splr(y=ys, svdH=X_r$svdH,  trace=F, J=7,
                                            thresh=1e-5, lambda=11.1, init = "naive",
                                            final.svd = T,maxit = 100, warm.start = NULL)
xbeta.sparse <- ys
xbeta.sparse@x = fit1$xbeta.obs
warm.start = propack.svd(naive_MC(as.matrix(xbeta.sparse)), X_r$rank)
start_time <- Sys.time()
for(i in 1:100){
fit2 = simpute.als.splr.fit.nocov.fixedJ(xbeta.sparse, X_r$rank, maxit=200, final.trim = F,thresh=1e-5,
                                          warm.start = warm.start, trace.it=F, return_obj = F)
}
print(paste("Execution time is",round(as.numeric(difftime(Sys.time(), start_time,units = "secs")),2), "seconds"))

xbeta_estim =  fit2$u %*% (fit2$d * t(fit2$v))
M = fit1$u %*% (fit1$d * t(fit1$v))
A_hat = M + xbeta_estim

print(paste("Test error Beta =", round(test_error(ginv(X_r$X)%*%xbeta_estim, gen.dat$beta.x),5)))
print(paste("Test error M =", round(test_error(M, gen.dat$B),5)))
print(paste("Test error A =", round(test_error((A_hat)[gen.dat$W==0], gen.dat$A[gen.dat$W==0]),5)))
```

### Pros

- Steps are faster since less calculations are required
- Provide a good solution (not the best)

### Cons

- It always take the same number of iterations no matter what the initial matrix is. This leads to slow run in cross-validation.
- Does not consider the covariate matrix $X$.
- Imputes the large matrix $X\beta$ of dimensions $(n\times m)$ instead of the low-rank matrix of interest $\beta$ of dimensions $(k \times m)$.


-------------------------------------

# Model 2

# SZIRCI

## Problem Set-up

We also assume that we have a partially observed matrix $Y$ of dimension $(n \times m)$ and rank $k$. We also fully observe matrix $X$ of dimension $(n\times k)$ and rank $k$. We seek a rank-k matrix $\beta$ suck that

$$
\| P_\Omega(Y) - P_\Omega(X{\beta}^T)\|^2_F < \epsilon,
$$

and 

$$
\|\beta\|_0 = k.
$$

Let the SVD decomposition of $\beta$ be $\beta=U_{\beta}D_{\beta}V_{\beta}^T$.


Define $U_x = \tilde{U}$ and $V_x = \tilde{D} \tilde{V}^T$ such that $X = \tilde{U}\tilde{D}\tilde{V}^T$.

Define $U = U_X V_X V_\beta$, $D=D_\beta$, and $V = U_\beta$ such that $X\beta^T = U D V^T$.

The sparse + low-rank decomposition is

$$
\begin{aligned}
Y^* &= P_\Omega(Y-A{\beta}^T) + U D V^T \\
 &= S + U_XV_XV_{\beta}D_{\beta}U_{\beta}^T
\end{aligned}
$$

The least squares solution for $\beta$ is

$$
\begin{aligned}
\beta &= (X^TX)^{-1}X^TY^* \\
&=(V_X^TU_X^TU_XV_X)^{-1}V_X^TU_X^T Y^* \\
&=(V_X^TV_X)^{-1}V_X^TU_X^T Y^* \\
&=(V_X^TV_X)^{-1}V_X^TU_X^T S + 
(V_X^TV_X)^{-1}V_X^TU_X^T U_XV_XV_{\beta}D_{\beta} U_\beta^T  \\
&=(V_X^TV_X)^{-1}V_X^TU_X^T S + 
(V_X^TV_X)^{-1}V_X^TV_XV_\beta D_\beta U_{\beta}^T  \\
\end{aligned}
$$

Since we don't update $X$ as it's fully observed, let $X_1=(V_X^TV_X)^{-1}V_X^TU_X^T$ and $X_2=(V_X^TV_X)^{-1}V_X^TV_X$. Then the LS solutions becomes:



$$
\begin{aligned}
\beta &=X_1 S + X_2V_\beta D_\beta U_\beta^T  \\
&=X_1 (Y-SPROD(XV_\beta ,U_\beta D_\beta )) + X_2V_\beta D_\beta U_\beta^T  \\
\end{aligned}
$$




Illustration:


```{r szirci, eval=TRUE}
# optional initialization. (if warm.start= NULL, the function will do this)
# first we generate simulation data and estimating the low-rank M and the observed Xbeta
setwd("/mnt/campus/math/research/kfouda/main/HEC/Youssef/HEC_MAO_COOP")
source("./code_files/import_lib.R")
gen.dat <- generate_simulation_data_ysf(2,800,700,10,10, missing_prob = 0.9,coll=T)
W_valid <- matrix.split.train.test(gen.dat$W, testp=0.2)
Y_train = (gen.dat$Y * W_valid)
Y_valid = gen.dat$Y[W_valid==0]
y = yfill = Y_train
y[y==0] = NA
ys <- as(y, "Incomplete")
X_r = reduced_hat_decomp(gen.dat$X, 1e-2)
beta_partial = MASS::ginv(t(X_r$X) %*% X_r$X) %*% t(X_r$X)
fit1 = CASMC_fit(y=ys, svdH=X_r$svdH,  trace=F, J=7,
                                            thresh=1e-5, lambda=11.1, init = "naive",
                                            final.svd = T,maxit = 100, warm.start = NULL)
xbeta.sparse <- ys
xbeta.sparse@x = fit1$xbeta.obs
X = X_r$X
svdX = fast.svd(X)#fast.svd(X)
Ux = svdX$u
Vx = svdX$d * t(svdX$v)
X0 = ginv(t(Vx)%*%Vx) %*% t(Vx)
warm.start = list()
warm.start$X1 = X0 %*% t(Ux)
warm.start$X2 = X0 %*% Vx
B = t( ginv(X) %*% naive_MC(as.matrix(xbeta.sparse))) # Beta = (X^-1 Y)'
warm.start$Bsvd = fast.svd(B)
#-----------------------------------------
start_time <- Sys.time()
for(i in 1:100){
fit3 = SZIRCI(xbeta.sparse, X_r$X, X_r$rank, final.trim = F,warm.start = warm.start,
                                 trace.it = F)
}
print(paste("Execution time is",round(as.numeric(difftime(Sys.time(), start_time,units = "secs")),2), "seconds"))
beta_estim2 =  fit3$u %*% (fit3$d * t(fit3$v))
M = fit1$u %*% (fit1$d * t(fit1$v))
A_hat2 = M + X_r$X %*% t(beta_estim2)
print(paste("Test error O =", round(test_error((A_hat2)[gen.dat$W==0], gen.dat$A[gen.dat$W==0]),5)))

print(paste("Test error Beta (Naive) =", round(test_error(ginv(X_r$X)%*%naive_MC(as.matrix(xbeta.sparse)),
                                                  gen.dat$beta.x),5)))
print(paste("Test error Beta (Model 2) =", round(test_error(t(beta_estim2), gen.dat$beta.x),5)))
```


Pros: 

- Similar performance to Model 1
- 3.6 times faster than model 1

Cons:

- Better performance than model 1 was expected
- Need to consider better ways of Initialization

----------------------------------------------------------------------------

## NEW: Comparing the performance of the second model (SZIRCI) compared to the original Soft-Impute model.

Model:  $Y = X \beta + \epsilon$ ,
where $X$ is a given covariate matrix, $\epsilon$ is a white noise that we're not interested in. 
Goal: Compute $\beta$
Input: A partially observed $Y$ (90% missing) and fully-observed $X$.
Assumption: No intercept. $X$ and $\beta$ have the same low rank $k$ which equals at most the number of covariates.
Replication: We will simulate the data and run the model 50 times

```{r comparison,  eval=TRUE}
B <- 50
with_noise = TRUE
error1 <- error2 <- numeric(B)
time1 <- time2 <- 0
for(b in 1:B){
   
   #if(b%%10==0) print(b)
   gen.dat <- generate_simulation_data_ysf(2,400,300,10,10, missing_prob = 0.9,coll=T,seed=b)
   X_r = reduced_hat_decomp(gen.dat$X, 1e-2)
   Y = A = X_r$X %*% gen.dat$beta
   if(with_noise) Y <- Y + matrix(rnorm(dim(Y)[1]*dim(Y)[2]),dim(Y)[1],dim(Y)[2])
   Y[gen.dat$W == 0] = NA 
   Y <- as(Y, "Incomplete")
   
   start_time <- Sys.time()
   fit = SZIRCI(Y, X_r$X,  X_r$rank, maxit=500,
                                    trace.it = F,final.trim = F,thresh = 1e-6)
   
   beta.estim <- fit$u %*% (fit$d * t(fit$v))
   A.estim <- X_r$X %*% t(beta.estim)
   error1[b] <- RMSE_error(A.estim[gen.dat$W==0], A[gen.dat$W==0])
   
   time1 <- time1 + as.numeric(difftime(Sys.time(), start_time,units = "secs"))
   
   start_time <- Sys.time()
   fit2 <- softImpute(Y, rank.max=X_r$rank, type="als", thresh=1e-6, trace.it=F,maxit = 500) 
   A.estim2 <- fit2$u %*% (fit2$d * t(fit2$v))
   error2[b] <- RMSE_error(A.estim2[gen.dat$W==0], A[gen.dat$W==0])
   
   time2 <- time2 + as.numeric(difftime(Sys.time(), start_time,units = "secs"))

}

print(paste0("The BetaImpute model has on average (with sd) rmse of ",round(mean(error1),4),
             "(",round(sd(error1),4),") and took ", round(time1),
             " seconds to run ", B, " simulations"))
print(paste0("The SoftImpute model has on average (with sd) rmse of ",round(mean(error2),4),
             "(",round(sd(error2),4),") and took ", round(time2),
             " seconds to run ", B, " simulations"))
```

