---
title: "Cooperative Learning"
author: "Khaled Fouda"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---


```{r setup, include=TRUE}
path_to_code <- "/mnt/campus/math/research/kfouda/main/HEC/Youssef/HEC_MAO_COOP/"
knitr::opts_chunk$set(echo = TRUE, eval = FALSE,  cache = TRUE)
knitr::opts_knit$set(root.dir=path_to_code)
```


```{r , include=TRUE, eval=TRUE, message=FALSE,warning=FALSE}
library(knitr)
library(kableExtra) 
library(tidyverse)
library(magrittr)
require(foreach)
require(doParallel)
source(paste0("./code_files/import_lib.R"))
```


### Model definition and Notations

Let the response vector $Y \in \Re^n$ and the covariate matrices $X \in \Re^{n\times p_x}$ and $Z \in \Re^{n \times p_z}$. The following quanitity is proposed for minimization

$$
\begin{aligned}
min E[\frac{1}{2}(y - f_x(X)-f_z(Z))^2+\frac{\rho}{2}(f_x(X)-f_z(Z))^2]&&(1)
\end{aligned}
$$
where the second term is the agreement penalty, encouraging the predictions from different views to agree. Equation 1 has fixed points:

$$
\begin{aligned}
f_x(X) &= E[\frac{y}{1+\rho} -\frac{1-\rho}{1+\rho}f_z(Z)|X] &&\\
f_z(Z) &= E[\frac{y}{1+\rho} -\frac{1-\rho}{1+\rho}f_x(X)|Z] && (2)
\end{aligned}
$$
Where the objective can be optimized by repeatedly updating the fit for each covariate matrix in turn, holding the other matrix fixed.

That is, we could use an alternative objective function to (1) that separates both covariate matrices as follows:

$$
\begin{aligned}
 &\| \frac{1}{1+\rho} (y+(1-\rho)f_z(Z)) - f_x(X)\|^2 + \|\frac{1}{1+\rho} (y+(1-\rho)f_x(X))-f_z(Z)\|^2& && (C1)
\end{aligned}
$$


#### Case: Regularized Linear Regression

Here, we assume that the columns of $X$ and $Z$ are standardized and y has mean zero. Hence, we can omit the intercept. The resulting objective function is:


$$
\begin{aligned}
J(\theta_x,\theta_z) = \frac{1}{2} \| y - X \theta_x - Z \theta_z \|^2 + \frac{\rho}{2} \|X\theta_x-Z\theta_z\|^2+\lambda_xP^x(\theta_x)+\lambda_zP^z(\theta_z) && (3)
\end{aligned}
$$

Where $P^x(\theta_x)$ can be any regularization term. Let's assume $L_1$ regularization in $\theta_x$ and $\theta_z$. Moreover, the author showed that generally, there is no advantage to allowing $\lambda_x$ and $\lambda_z$ be different. The only case where it was appropriate to make them different is when one of the covariate matrices doen't contribute at all to the model. We will assume them equal for now, as did the author.

$$
\begin{aligned}
J(\theta_x,\theta_z) = \frac{1}{2} \| y - X \theta_x - Z \theta_z \|^2 + \frac{\rho}{2} \|X\theta_x-Z\theta_z\|^2+\lambda (\|\theta_x\|+\|\theta_z\|) && (5)
\end{aligned}
$$

We could rewrite equation 5 as:

$$
\begin{aligned}
J(\theta_x,\theta_z) = \frac{1}{2} \| \tilde y - \tilde{X}\tilde\beta \|^2 +\lambda (\|\theta_x\|+\|\theta_z\|) && (7)
\end{aligned}
$$
where


$$
\begin{aligned}
\tilde X = \matrix{X&Z\\-\sqrt \rho Z&\sqrt \rho X} && \tilde y = \matrix{y\\0} && \tilde \beta = \matrix{\theta_x\\\theta_z}&& (6)
\end{aligned}
$$
Equation 5 is a form of lasso which then we could use the glmnet package to git the model. 
Let the $Lasso(X,y,\lambda)$ denote the generic problem:

$$
\begin{aligned}
min_\beta \frac{1}{2} \| y - X\beta\|^2 + \lambda \| \beta\| && (8)
\end{aligned}
$$
We could also incorporate $L_2$ penalties (elastic net regularization) to the objective in 5. This option is included in their software implementation.


$$
\begin{aligned}
\lambda[ (1-\alpha)(\|\theta_x\|+\|\theta_z\|)+\alpha(\|\theta_x\|^2_2/2+\|\theta_z\|^2_2/2) ] && (9)
\end{aligned}
$$

### Solution

The author provided two algorithms to solve equation 5.

#### Algorithm 1: Direct Calculation


for each $\rho$ in the grid, we solve  $Lasso(\tilde X,\tilde y,\lambda)$ over a decreasing grid of $\lambda$.

#### Algorithm 2: one-at-a-time for regularized linear regression

the updates are (for each $\lambda_x$, $\lambda_z$, and $\rho$ in the grid):

$$
\begin{aligned}
\hat\theta_x &= Lasso(X,y_x^*,\lambda_x) && where && y_x^*=\frac{y}{1+p}-\frac{(1-p)Z\theta_z}{1+p},\\ 
\hat\theta_z &= Lasso(X,y_z^*,\lambda_z) && where && y_z^*=\frac{y}{1+p}-\frac{(1-p)X\theta_x}{1+p}.
\end{aligned} 
$$

## Code and R package

The simulation and implementation code can be found at

https://github.com/dingdaisy/cooperative-learning

and the R package Multiview at https://cran.r-project.org/web/packages/multiview/. The package includes functions to apply cooperative learning on two or more views using linear, poisson, and logistic regression. The package uses glmnet. The code for the functions can also be found in the github.

----------------------------
---------------------------

# Mao-Coop Model


The following model is introduced by Youssef in page 4 of his notes.

\begin{equation}
    A = (XA + \Theta_X) + (ZB + \Theta_Z)^T,\\
    Y = (A+E) * W
\end{equation}


with the following loss function

$$
\begin{align}
    \text{Loss-3} =& \left\|  Y - (XA + \Theta_X) - (ZB + \Theta_Z)^T \right\|_F^2 +\\& \rho \left\| (XA + \Theta_X) - (ZB + \Theta_Z)^T \right\|_F^2 +\\
    & \lambda_1 \left\| A \right\|_F^2 + \lambda_2 \left( (1 - \alpha_1) \left\| \Theta_X \right\|_F^2 + \alpha_1 \left\| \Theta_X \right\|_* \right)+\\
    & \lambda_3 \left\| B \right\|_F^2 + \lambda_4 \left( (1 - \alpha_2) \left\| \Theta_Z \right\|_F^2 + \alpha_2 \left\| \Theta_Z \right\|_* \right).&&(Y0)
\end{align}
$$


with $\Theta_X$ and $\Theta_Z$ low rank matrices that we hope it will capture the part of $Y$ that is not linearly explained by either $X$ or $Z$. Furthermore, assume that the column spaces of $X$ and $\Theta_X$ are orthogonal, and that the column spaces of $Z$ and $\Theta_Z$ are orthogonal. 

To get the quantities of interest, a possible approach is to solve in an iterative way the following

$$
\begin{aligned}
\hat{A}, \hat{\Theta}_X =& \arg \min_{A,\Theta_X} \left\| \frac{1}{1 + \rho}  Y - \frac{1 - \rho}{1 + \rho} (ZB + \Theta_Z)^T - (XA + \Theta_X) \right\|_F^2 + \\
 &\lambda_1 \left\| A \right\|_F^2 + \lambda_2 \left((1 - \alpha_1) \left\| \Theta_X \right\|_F^2 + \alpha_1 \left\| \Theta_X \right\|_* \right), && (Y1) \\
\hat{Z}, \hat{\Theta}_Z =& \arg \min_{B,\Theta_Z} \left\| \frac{1}{1 + \rho} Y^T - \frac{1 - \rho}{1 + \rho} (XA + \Theta_X)^T - (ZB + \Theta_Z) \right\|_F^2 +\\
 &\lambda_3 \left\| B \right\|_F^2 + \lambda_4 \left((1 - \alpha_2) \left\| \Theta_Z \right\|_F^2 + \alpha_2 \left\| \Theta_Z \right\|_* \right). && (Y2)
\end{aligned}
$$

### Interpretation of the solution

The the first two terms in the loss in (Y0) has the same form as in Mao's formula and hence if we replace these terms with following two terms

$$
\| f_x(X) - \frac{y}{1+\rho} - \frac{1-\rho}{1+\rho}f_z(Z)\|^2 \\
\| f_z(Z) - \frac{y}{1+\rho} - \frac{1-\rho}{1+\rho}f_x(x)\|^2
$$
where in our case $f_x(X)=XA+\Theta_X$ and $f_z(Z)=(ZB+\Theta_Z)^T$. 

### Connection to Mao

In order to iteratively solve (Y1) and (Y2) using an a method similar to the one-at-a-time algorithm, we will apply Mao method to minimize each formula. First, we need to have them in the form of equations (4) and (5) from Mao's paper (check the other html file).

The regularization terms match exactly the form and hence we don't need to adjust them. 
For the first term in (Y1), let 
$$
\begin{aligned}
Y^* = \frac{\ Y}{1+\rho} - \frac{1-\rho}{1+\rho}\hat {f_z}(Z) = \frac{Y}{1+\rho} - \frac{1-\rho}{1+\rho}(Z\hat{B}+\hat{\Theta}_Z)^T && (Y3)
\end{aligned}
$$
where $\hat {f_z}(Z) = Z\hat{B}+\hat{\Theta}_Z$ are the estimates from fitting (Y2) in the previous iteration.  

Similarly, for the first term in (Y2), let

$$
\begin{aligned}
Y^* = \frac{ Y}{1+\rho} - \frac{1-\rho}{1+\rho}\hat {f_x}(x) = \frac{Y}{1+\rho} - \frac{1-\rho}{1+\rho}(X\hat{A}+\hat{\Theta}_X) && (Y4)
\end{aligned}
$$
where $\hat {f_x}(X) = X\hat{A}+\hat{\Theta}_X$ are the estimates from fitting (Y1) in the previous iteration.



### Implementation 

#### Note:

- When $\rho=1$ The adjusted amount is $Y^*=\frac{1}{2}Y$ and hence the solution is the average of the marginal fit of both models. However, as we see below, the solution is always better than the average of the marginal fit.

- The formula above with $Y= (A+E)*W$ means that all entities in $Y^*$ will be non-zero including the missing positions, due to the adjustment term $-\frac{1-\rho}{1+\rho}\hat f$. The solution using this method didn't converge in many of the simulation. Instead, we attempted to use  $Y^*= Y^* *W$ where we set all the missing values with 0 again. After testing the method, we found that the results were better than before but there were few cases where the method didn't converge as well.
Finally, we attempted (as been done by Youssef) to use the original $Y^*$ for the Mao cross-validation function and $Y^**W$ for the Mao fit function to make the prediction. This method achieved the best results and all our simulations converged. 

- The error function used for both my simulations and Yousef's is the same as one used in Mao:

$$
Error = \frac{\sum_{i,j \in test}{(\hat{A}_{ij}-A_{ij})^2}}{\sum_{i,j \in test} (A_{ij}-\bar{A})^2}
$$


---------


### Estimation (Added after the meeting) (Is it supposed to be always like this?)

The estimated matrix $\hat{A}$ is the sum of estimated matrices from both $X$ and $Z$, that is

$$
\begin{aligned}
\hat{A} = \hat{A_Z}+\hat{A_X} = X\hat{A}+\hat{\Theta}_X + (Z\hat{B}+\hat{\Theta}_Z)^T && (Y5)
\end{aligned}
$$
For each iteration, we fit 2 Mao models with $Y^*$ instead $Y$ as the response. Therefore, the estimated matrices $\hat{A}_Z$ and $\hat{A}_Z$ apporximate the $Y^*$ corresponding to them. Therefore, the forumla for estimation above might not represent the true weights of each of the two estimated matrices. 
For example, for $\rho=1$, $Y^*=\frac{1}{2}Y$ will always be the case and hence $\hat{A} = \hat{A_Z}+\hat{A_X}$ is true. But when $\rho$ is not 1, then it gets more complicated. 

One solution comes to mind is to solve for $Y$ in (Y3) and (Y4) equations above. Let the estimation of $Y^*$ in (Y3) be $\hat{A}_X$ and the estimation $Y^*$ in (Y4) be $\hat{A}_Z$, the solution to the first equation is:

$$
\begin{aligned}
\hat{Y} = (1-p) \hat{A}_X + (1+p) \hat{A}_Z 
\end{aligned}
$$
Where $\hat{A}_Z$ is the output of fitting the Mao model to $Z$ in the previous step and $\hat{A}_X$ is the output of fitting the Mao model to $X$ in the current step. Similarly for the second equation (Y4):

$$
\begin{aligned}
\hat{Y} = (1-p) \hat{A}_Z + (1+p) \hat{A}_X
\end{aligned}
$$

Where $\hat{A}_X$ is the output of fitting the Mao model to $X$ in the previous step and $\hat{A}_Z$ is the output of fitting the Mao model to $Z$ in the current step.

My suggestion is to take the average from both solutions to get the final estimate. I will implement this method as a next step and compare the results. For the results below, I used the sum in (Y5) to compute the results.

-----------


```{r code=readLines(paste0(path_to_code, "/Mao_Coop_fit.R")),eval=TRUE}

```


```{r eval=TRUE}

gen.dat <- generate_simulation_data_ysf(1,300,300,5,5, missing_prob =0.8 ,coll=TRUE)

A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,30,1e-6,1,alpha_grid = c(1),numCores=1)
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,30,1e-6,0.99,alpha_grid = c(1),numCores=1)
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,30,1e-6,0.98,alpha_grid = c(1),numCores=1)
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,30,1e-6,0.9,alpha_grid = c(1),numCores=1)
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,30,1e-6,0.7,alpha_grid = c(1),numCores=1)
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,30,1e-12,0.5,alpha_grid = c(1),numCores=1)
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,30,1e-6,0.3,alpha_grid = c(1),numCores=1)
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,30,1e-6,0.01,alpha_grid = c(1),numCores=1)

```


I slightly modified Youssef's implementation so that the simulation parameters, error function, and the seed are equivalent. I ran his Mao_coop.R file and copied the output below.


```{r code=readLines(paste0(path_to_data, "/Youssef_Mao_Coop_Results.txt"))}

```

Testing the algorithm with larger matrix size.

```{r test2, eval=TRUE}
gen.dat <- generate_simulation_data_ysf(1,500,500,5,10, missing_prob =0.8 ,coll=TRUE, seed=3023)

A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,20,1e-6,1,2,alpha_grid = c(1),numCores=1, seed=3023)
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,20,1e-6,0.99,2,alpha_grid = c(1),numCores=1, seed=3023)
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,20,1e-6,0.97,2,alpha_grid = c(1),numCores=1, seed=3023)
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,20,1e-6,0.95,2,alpha_grid = c(1),numCores=1, seed=3023)
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,20,1e-6,0.9,2,alpha_grid = c(1),numCores=1, seed=3023)
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,20,1e-6,0.7,2,alpha_grid = c(1),numCores=1, seed=3023)
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,20,1e-6,0.3,2,alpha_grid = c(1),numCores=1, seed=3023)
```



$$
\begin{aligned}
\end{aligned}
$$