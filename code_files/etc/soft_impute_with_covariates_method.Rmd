---
title: "Soft Impute With Covariates"
author: "Khaled Fouda"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=TRUE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, eval = FALSE)
library(kableExtra)
library(magrittr)
library(tidyverse)
knitr::opts_knit$set(root.dir="/mnt/campus/math/research/kfouda/main/HEC/Youssef/HEC_MAO_COOP/")
#source("./Mao/SMC_functions6.R")
```


```{r include=TRUE, warning=FALSE, message=FALSE, eval=TRUE}
source("./code_files/Mao_import_lib.R")
data_dir = "./Mao/saved_data/"
```


References: 
   - Spectral Regularization Algorithms for Learning Large Incomplete Matrices. Mazumder & Hastie 2010
   

Soft Impute attempts to recover a partially observed matrix without covariates while minimizing the rank. There are 3 variates of the method depending on the type of the regularization term. The first one, called SOFT-IMPUTE, uses the Nuclear Norm Regularization. Assuming the same notations as always, A is the full matrix we want to recover. Y is the noisy version of A and we partially observe Y. W is the mask where $W_{ij}=1$ if $Y_{ij}$ is observed. Consider $\Omega$ the set of observed indices in $Y$.  We also consider the complementary projection $P_{\over{\Omega}}(Y) = Y - P_{\Omega}(Y)$. Finally, the objective function is as follows:

$$
\begin{aligned}
{minimize}_{B} && \frac{1}{2} \|Y-B\|^2_F + \lambda\|B\|_* && (8)
\end{aligned}
$$
which has the following solution

$$
\begin{aligned}
\hat{B} = S_\lambda(Y) = UD_\lambda V' && with&& D_\lambda = diag[(d_1-\lambda)_+(d_2-\lambda)_+\cdots(d_r-\lambda)_+] && (9)
\end{aligned}
$$
Using the SVD of $Y=UD V'$. The rank of $Y$ is assumed to be $r$. The notation $S_\lambda(Y)$ refers to soft-thresholding. 

Equation (8) is equivalent to 


$$
\begin{aligned}
&&{minimize}_{B} && \frac{1}{2} \|P_{{\Omega}}(Y)-P_{{\Omega}}(B)\|^2_F + \lambda\|B\|_* && (10)\\
= &&{minimize}_{B} && \frac{1}{2} \|P_{{\Omega}}(Y)+P_{\over{\Omega}}(B)-B\|^2_F + \lambda\|B\|_* && 
\end{aligned}
$$

***********
**Algorithm 1** SOFT-IMPUTE

1. **Initialize** $B^{old} = 0$.
2. **Do for** $\lambda_1 > \lambda_2 > \ldots > \lambda_k$:

    a. **Repeat**:
    
        i. **Compute** $B^{new} \leftarrow S_{\lambda_k}(P_{\Omega}(Y) + P^{\perp}_{\Omega}(B^{old}))$.
        
        ii. **If** $\frac{\|B^{new} - B^{old}\|_F}{\|B^{old}\|_F} < \epsilon$ **exit**.
        
        iii. **Assign** $B^{old} \leftarrow B^{new}$.
    
    b. **Assign** $B_{\lambda_k} \leftarrow B^{new}$.

3. **Output** the sequence of solutions $\hat{B}_{\lambda_1}, \ldots, \hat{B}_{\lambda_k}$.

************************

In terms of computations, the update inside the loop is as follows:


1. Initialization $Y^* = Y,\cdots$.
2. **Do for** $\lambda_1 > \lambda_2 > \ldots > \lambda_k$:

    a. **Repeat**:
         
        i. svd.old = svd
        
        ii. $\hat{Y} = S_{\lambda_k}(P_{\Omega}(Y) + P^{\perp}_{\Omega}(B^{old})) = U[,:J](D[,:J]-\lambda)V[,:J]'$
        
        iii. $Y^*[\overline{\Omega}] = \hat{Y}[\overline{\Omega}]$
        
        iv. svd = svd($Y^*$)
        
    
    b. **Assign** $B_{\lambda_k} \leftarrow Y^*$.

3. **Output** the sequence of solutions $\hat{B}_{\lambda_1}, \ldots, \hat{B}_{\lambda_k}$.

***********************************







```{r}
gen.dat <- generate_simulation_data_ysf(2,600,600,10,10, missing_prob = 0.9,coll=FALSE)
gen.dat$Y[gen.dat$W==0] = NA
sout <- softImpute(gen.dat$Y, lambda=30, rank.max=10, trace.it = TRUE, thresh=1e-3)


sout <- simpute.svd.cov(gen.dat$Y, gen.dat$X, thresh=1e-3, lambda = 30, J=10, trace.it = TRUE)


W_valid <- matrix.split.train.test(gen.dat$W, testp=0.2)

sout1 <- simpute.svd.cov.cv(gen.dat$Y*W_valid, gen.dat$X, W_valid, gen.dat$Y, trace=TRUE, rank.limit = 30)
test_error(sout1$A_hat[gen.dat$W==0], gen.dat$A[gen.dat$W==0])
test_error(sout1$beta_hat, gen.dat$beta)
test_error(sout1$B_hat, gen.dat$B)

sout <- simpute.orig(gen.dat$Y*W_valid, W_valid, gen.dat$Y, trace=TRUE, rank.limit = 30)
test_error(sout$A_hat[gen.dat$W==0], gen.dat$A[gen.dat$W==0])


sout$d

sout1$rank_A
sout1$rank.max
qr(co)$rank

co <- complete(gen.dat$Y, sout, FALSE)

v=as.matrix(sout$v)
   vd=v*outer(rep(1,nrow(v)),sout$d)
   soft_estim = sout$u %*% t(vd)
qr(soft_estim)$rank

object <- sout
i <- dim(object$v)


v=as.matrix(object$v)
 vd=v*outer(rep(1,nrow(v)),object$d)
 out= suv(as.matrix(object$u),vd,i,j)

```



```{Rcpp}
subroutine suv (nrow,ncol,nrank,u,v,irow,jcol,nomega,r)
      integer nrow, ncol, nrank,nomega, irow(nomega),jcol(nomega)
      double precision u(nrow,nrank),v(ncol,nrank),r(nomega)
c      double precision rtemp
c      computes uv'[i,j]=<u[i,],v[j,]>
      integer ii,jj
!HPF$ INDEPENDENT
      do 10 i=1,nomega
         ii=irow(i)
         jj=jcol(i)
      r(i) = DOT_PRODUCT(u(ii,:), v(jj,:))
c          r(i)=rtemp
10    continue
      return
      end

