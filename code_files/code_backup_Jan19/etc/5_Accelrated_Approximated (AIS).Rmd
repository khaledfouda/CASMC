---
title: "Accelerated Convergence and Approximated SVD"
author: "Khaled Fouda"
date: "`r Sys.Date()`"
output: html_document
---


```{r setup, include=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = T, include=TRUE, cache = TRUE, results = "hold")
library(kableExtra)
library(magrittr)
library(tidyverse)
knitr::opts_knit$set(root.dir="/mnt/campus/math/research/kfouda/main/HEC/Youssef/HEC_MAO_COOP/")
```


```{r include=TRUE, message=FALSE, warning=FALSE}
source("./code_files/Mao_import_lib.R")
data_dir = "./Mao/saved_data/"

```


# Accelerated Convergence and Approximated SVD

-----------

## UPDATES

I have implemented this method in R and found that:

1. The performance without covariates is better than ALS
2. The performance with covariates is bad
3. It is much slower than ALS

Hence I decided to drop this method for now.


--------------

## References:

- main

   [1] ["Accelerated and Inexact Soft-Impute for Large-Scale Matrix Completion" Q.Yao and J.Kwok. IEEE             2019](https://www.computer.org/csdl/journal/tk/2019/09/08449114/13rRUytWF9Q)
   
- Extra

   [2] ["FINDING STRUCTURE WITH RANDOMNESS: PROBABILISTIC ALGORITHMS FOR CONSTRUCTING APPROXIMATE MATRIX DECOMPOSITIONS"](https://arxiv.org/pdf/0909.4061.pdf) 
   2010 - for approximating SVT decomposition
   
   [3] ["Proximal Gradient Descent and Acceleration"](https://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/prox-grad.pdf) Lecture notes on Proximal Algorithms by Tibshirani 

In this paper the authors considered two main modification to the Soft-Impute method:

1. Accelrating the convergence rate by using the power method (an iterative algorithm to find an approximation to the largest dominant eigenvalue and the corresponding eigenvector). 

2. To improve the algorithm even more, they approximated the SVD decomposition by representing the loss function as two two separable functions (error+regularization) and using proximal algorithms for optimization given that our loss function is convex. They reported that accelartion has improved the convergence rate from $O(1/T)$ in Soft-Impute to $O(1/T^2)$, where $T$ is the number of iterations.


Consider our usual model without covariates


$$
\begin{aligned}
minimize_M && F(M) = \frac{1}{2} \|Y-M\|^2_F + \lambda ||M||_*
\end{aligned}
$$




which as the following solution



$$
\begin{aligned}
\hat{M} = SVT_\lambda(Y) = U D_\lambda V'&& with && D_\lambda = diag[(d_1-\lambda)_+\cdots(d_r-\lambda)_+]
\end{aligned}
$$

## Part 1: Acceleration

Consider the partition of


$$
\begin{aligned}
&F(M) = f(M) + g(M) \\
&f(M) = \frac{1}{2} \|Y-M\|^2_F \\
&g(M) = \lambda ||M||_*
\end{aligned}
$$
We want to minimize $F(M)$. The proximal algorithms provide a framework to solve this problem considering that both f, g are convex, and f is smooth but g is possibly non-smooth. The proximal algorithm generates a sequence of estimates $\{M_t\}$ as:


$$
\begin{aligned}
M_{t+1} = prox_{\mu g}(Y_t) = arg min_M && \frac{1}{2} \|Y_t-M\|^2_F + \mu g(M),
\end{aligned}
$$
where

$$
\begin{aligned}
&Y_t = Z_t - \mu \Delta f(Z_t),\\
&Z_t = (1+\theta_t) M_t - \theta_t M_{t-1},\\
& \theta_{t+1} = \frac{t-1}{t+2}
\end{aligned}
$$

and $\mu \le 1/\rho$ being a fixed step size and $f$ is $\rho$-Lipshitz smooth($\|\Delta f(M_1)-\Delta f(M_2)\| \le \rho \| M_1 - M_2\|$). However, since $f(M)$ is 1-Lipshitz smooth, we can set the step size to any value below or equal 1. Let's set it up to 1. Note that when $\mu=1$, then $prox_{g}(Y_t) = SVT_{\lambda}(Y_t)$, which means that Soft-Impute is a type of proximal algorithms.

Moreover, note that $\Delta f(M) =  M-Y$

The new update equations are then:

$$
\begin{aligned}
(1)&&M_{t+1} &= prox_{g}(Y_t) = SVT_\lambda(Y_t) = arg min_M \frac{1}{2} \|Y_t-M\|^2_F + \lambda ||M||_*,\\
(2)&& \theta_{t+1} &= \frac{t-1}{t+2},\\
(3)&&Z_t &= (1+\theta_t) M_t - \theta_t M_{t-1},\\
(4)&& Y_t &= P_\Omega(Y-Z_t) + Z_t \text{ (ie  } Y_t[\overline \Omega] = Z_t[\overline \Omega],Y_t[\Omega] = Y[\Omega] \text{ )}.
\end{aligned}
$$
Note that the difference between this and the Soft-Impute is in the extra steps of (2) and (3). In Soft-Impute, we set $\theta_t=0 \; \; \forall t$.  The accelerated algorithm has a slightly higher iteration complexity that the soft-impute. However, this is compensated by improving the convergence rate. The authors reported a convergence rate of $O(1/T^2)$ compared to a convergence rate of $O(1/T)$ for both soft-Impute and ALS-Impute. 

The methods still requires the computation of the SVT of a large matrix. Part 2 provides an approximation method for step (1) instead of computing the exact SVT.


## Part 2: SVT Approximation

Reference 3 provide multiple methods to approximate the SVD or the QR decomposition of matrices. Assume that we have a matrix $A$ of large dimensions $m \times n$ and we want to approximate $A=U\Sigma V^T$. The algorithm consists of two stages:

   1. the first stage is to compute a matrix $Q$ of dimensions $m \times k$ where k is a much lower dimension and that $Q$ is the basis of A (has orthonormal columns and $A \approx Q Q^TA$). 
   
   2. The second stage  Computes the SVD decomposition of $Q^TA \in \Re^{k\times m} = [\tilde U,\Sigma,V]$
      - set $U \leftarrow Q \tilde U$
      - $\hat{A} = U \Sigma_\lambda V^T$
      

The proposed algorithm for stage 1 (by Yao et al.) is as follows:

---
title: "Algorithm in R Markdown"
output: pdf_document
---

*********************************************************************

**Algorithm 1: Approximation -  Stage 1**

*************************************************************************

   **Require**: $A \in \mathbb{R}^{m \times n}$, $R \in \mathbb{R}^{n \times k}$, and the number of iterations $J$;

   1. Initialize $Q_0$ with QR factorization of $AR$:
   
      $$Q_0 = QR(AR)$$
   
      where QR(.) denotes the QR factorization.

   2. For $j = 1$ to $J$ do:

      a. Update $Q_j$ with QR factorization of $A{A}^T Q_{j-1}$:
   
         $$Q_j = QR(A{A}^T Q_{j-1})$$
   
   3. Return $Q_J$.

*******************************************************************************



*********************************************************************

**Algorithm 2: Approximation -  Stage 2**

*************************************************************************

   **Require**: ${A} \in \mathbb{R}^{m \times n}$, $R \in \mathbb{R}^{n \times k}$, and $\lambda \geq 0$;

   1. Compute $Q$ using the Algorithm 1:
      
      $$Q = \text{Algorithm1}(A, R, J);$$
   
   2. Perform SVT on $Q^T A$:
      
      $$[\tilde U, \Sigma_\lambda, V] = \text{SVT}(Q^T A);$$
   
   3. Return the matrices $Q$, $U=Q\tilde U$, $\Sigma_\lambda$, and $V$. The approximated $\hat{A}$ is given by:
      
      $$\hat{A} = U\Sigma_\lambda V^T;$$
      
*************************************************************************************



## Putting it all together

## Accelerated Inexact Soft-Impute (AIS-Impute) Algorithm


******************************************************************

**Algorithm 3** Accelerated Inexact Soft-Impute (AIS-Impute).

**********************************************************************

**Require**: partially observed matrix $Y$, parameter $\lambda$.

1. Initialize $c = 1$, $M_0 = M_1 = 0$, step-size , $\hat{Y}=Y$ $\lambda > \hat{\lambda}$ and $\nu \in (0, 1)$;
2. For $t = 1, 2, \ldots, T$ do:
3. $\quad \lambda_t = (\lambda - \hat{\lambda})\nu^{t-1} + \hat{\lambda}$;
4. $\quad Z_t = M_t + \theta_t (M_t - M_{t-1})$, where $\theta_t = \frac{c-1}{c+2}$;
5. $\quad \hat{Y}[\overline\Omega] = Z_t[\overline\Omega]$;
6. $\quad V_{t-1} = V_{t-1} - V_t (V_t^T V_{t-1})$, remove zero columns;
7. $\quad R_t = QR([V_t, V_{t-1}])$;
8. $\quad [U_{t+1}, \Sigma_{t+1}, V_{t+1}] = \text{approx-SVT}(\hat{Y}, R_t, \lambda_t, J)$;
9. $\quad \text{// } M_{t+1} = U_{t+1}\Sigma_{t+1}V_{t+1}^T$
10. $\quad \text{if } F(M_{t+1}) > F(M_t) \text{ then } c = 1$;
11. $\quad \text{else } c = c + 1; \text{ end if}$
12. $\quad \text{end for}$
13. Return $U_{T+1}, \Sigma_{T+1}$ and $V_{T+1}$.

**************************************************************************

## Original Implementation

The method and simulations are fully implemented in Matlab and available on the author's github: https://github.com/quanmingyao/AIS-impute/tree/master


---------------------------------------------------------------------------------------------------------