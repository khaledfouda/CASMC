---
title: "3 - Alternating Minimization Algorithms (ALS)"
author: "Khaled Fouda"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = T, include=TRUE, cache = TRUE, results = "hold")
library(kableExtra)
library(magrittr)
library(tidyverse)
knitr::opts_knit$set(root.dir="/mnt/campus/math/research/kfouda/main/HEC/Youssef/HEC_MAO_COOP/")
```


```{r include=TRUE, message=FALSE, warning=FALSE}
source("./code_files/Mao_import_lib.R")
data_dir = "./Mao/saved_data/"

```

## References:

[1] Hastie (2015) Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares



Building on our model in the document "Soft Impute With Covariates", we explore alternative ways of computing the low-rank matrix "M" while achieving the same results. The alternative ways are much faster that they outperform Mao's model.

We begin by introducing the ALS algorithm for estimating M, first without covariates:

Consider our usual model (model 8 in Soft Impute With Covariates):

$$
\begin{aligned}
minimize_M && \frac{1}{2} \|Y-M\|^2_F + \lambda ||M||_*
\end{aligned}
$$
which as the following solution



$$
\begin{aligned}
\hat{M} = S_\lambda(Y) = U D_\lambda V'&& with && D_\lambda = diag[(d_1-\lambda)_+\cdots(d_r-\lambda)_+]
\end{aligned}
$$
where $r$ acts as an upper bound to the rank of $\hat{M}$. Since we don't require the full SVD decomposition to compute $\hat{M}$ and we only need the rows/columns up-to $r$, the authors have discussed the possibility of using alternative methods to do partial svd decomposition. Specifically, they discussed the use of the algorithm "PROPABK" which is an state-of-the-art method of computing SVD decomposition up-to certain number of Eigen Values. However, they didn't do it in their implementation and they used full SVD decomposition instead. 


A second trick they proposed to avoid the high computations is that they rewrote $\hat{Y}$ as 

$$
\begin{aligned}
\hat{Y} = [P_\Omega(Y) - P_\Omega(\hat{M})]  + \hat{M}
\end{aligned}
$$

$\hat{Y}$ ubove refers to the filled matrix where observed values are left as they are and missing values are filled from $\hat{M}$. The first term above is a very sparse matrix since the missing values are left as 0. The second term is not sparse but is low-rank. Using the two terms instead of $\hat{Y}$ should improve the speed of matrix operations. Again, they proposed it but didn't use it. I attempt to use it but it provided no significant improvement over using $\hat{Y}$ as is. Maybe this will be different for extremely large matrices as I tested with dimensions up-to $1000\times 1000$.

Hastie et al., and based on the approach by Rennie and Srebero (2005), they considered the following model instead


$$
\begin{aligned}
minimize_{A,B} && \frac{1}{2} \|Y-AB^T\|^2_F + \frac{\lambda}{2} (||A||^2_F+||B||^2_F)  && (6)
\end{aligned}
$$
Where $A$ is $n_1\times r$ and $B$ is $n_2 \times r$ and $M=AB^T$. The loss above is called maximum-margin matrix factorization (MMMF) which is not convex in A and B, but is bi-convex -- for fixed B, the loss is convex in A and vice-versa. If the solution to our problem has a rank $q \le r$ then the solution to this problem is 


$$
\begin{aligned}
\hat{A} &= U_r S_\lambda(D_r)^{\frac{1}{2}} && (7)\\
\hat{B} &= V_r S_\lambda(D_r)^{\frac{1}{2}}, &&
\end{aligned}
$$

This method could still make use of the sparse + low-rank property. 

Lemma 1 of theorem 2 states the following:

$$
\| M\|_* = min_{A,B:M=AB'} \frac{1}{2}(\|A\|^2_F+\|B\|^2_F)
$$

Suppose we have current estimates of $A$ and $B$ and we wish to compute the new $\tilde{B}$. We will use a filled $Y^*$ using the estimates from the previous iterations and

$$
\begin{aligned}
minimize_{B} && \frac{1}{2} \|Y^*-A\tilde B^T\|^2_F + \lambda ||\tilde B||^2_F.
\end{aligned}
$$
With $Y^* =  P_{\Omega}(Y) + P_{\overline \Omega}(AB') = (P_{\Omega}(Y) + P_{\Omega}(AB')) + AB'$

Which is again, a sparse + low-rank.




*************************************************

**Algorithm 3.1 Rank-Restricted Efficient Maximum-Margin Matrix Factorization: softImpute-ALS**

1. Initialize $A = UD$ where $U_{n_1 \times r}$ is a randomly chosen matrix with orthonormal columns
   and $D = I_r$, the $r \times r$ identity matrix, and $B = VD$ with $V = 0$. Alternatively, any
   prior solution $A = UD$ and $B = VD$ could be used as a warm start.
2. Given $A = UD$ and $B = VD$, approximately solve the following to update $B$:

   $$minimize_B \frac{1}{2} \|P_{\Omega}(Y) - AB^T\|_F^2 + \frac{\lambda}{2} \|B\|_F^2$$

   We achieve that with the following steps:
   - (a) Let $Y^* = (P_{\Omega}(Y) - P_{\Omega}(AB^T)) + AB^T$, stored as sparse plus low-rank.
   - (b) Solve the optimization problem:

     $$minimize_M \frac{1}{2} \|Y^* - AB^T\|_F^2 + \frac{\lambda}{2} \|B\|_F^2$$

     with the solution:

     $$B^T = (D^2 + \lambda I)^{-1}DU^TY^*$$
     $$= (D^2 + \lambda I)^{-1}DU^T(P_{\Omega}(Y) - P_{\Omega}(AB^T))$$
     $$+ (D^2 + \lambda I)^{-1}D^2B^T.$$

   - (c) Use this solution for $\bar{B}$ and update $V$ and $D$:
     1. Bompute the SVD decomposition $\bar{B}D = \tilde{U}\tilde{D}\tilde{V}^T$.
     2. Update $V \leftarrow \tilde{U}$, and $D \leftarrow \tilde{D}$.

3. Given $B = VD$, solve for $A$. My symmetry, this is equivalent to step 2, with $Y^T$ replacing $Y$, and $B$ and $A$ interchanged.
4. Repeat steps (2)-(3) until convergence.
5. Bompute $M = Y^*V$, and then its SVD: $M = U_{\sigma}D_{\sigma}R^T$. Then output $U, V \leftarrow VR$ and $D_{\lambda} = \text{diag}(\sigma_1 - \lambda, \ldots, \sigma_r - \lambda)_+$.

*****************************************************************

**In other words, at iteration i,** we have previous estimates of $A=UD$ and $B=VD$ as well as previous estimates of $U,D,V$.

1. Solve for $B$ as:

   -  $DB^T = (D^2 + \lambda I)^{-1}D^2U^TY^* = (D^2/(D^2+\lambda))U^TY^*$
   -  $\bar{B}D = \tilde{U}\tilde{D}^2\tilde{V}^T$
      -  $V \leftarrow \tilde{U}$
      -  $D^2 \leftarrow \tilde{D^2}$
   -  $Y^*[\overline\Omega] = AM^T[\overline{\Omega}] = UD^2V^T[\overline \Omega]$
2. Solve for $A$ as:
   -  $DA^T = (D^2 + \lambda I)^{-1}DV^T{Y^*}^T =  (D^2/(D^2+\lambda))(VY^*)^T$
   - $\bar{A}D = \tilde{U}\tilde{D}^2\tilde{V}^T$
      -  $U \leftarrow \tilde{U}$
      -  $D^2 \leftarrow \tilde{D^2}$
   - $Y^*[\overline\Omega] = AM^T[\overline{\Omega}] = UD^2V^T[\overline \Omega]$
3. Compute Frobenious ratio to check for convergence $Frob(U_{old}, D^2_{old}, V_{old}, U, D^2, V)$

4. Iterate over steps 1-3 till convergence.

5. Compute the SVD decomposition  $Y^*V=U D_\sigma^2 V$
   - $D^2 = diag((\sigma_1-\lambda)_+,\cdots,(\sigma_r-\lambda)_+)$
   - $\hat{M} = \hat{Y} = U D^2 V^T$

**************************************************************

Notice that in the calculations above we used $D^2$ and we didn't need to store $D$. Moreover, We don't need to store $A$ and $B$ and we could replace them with $U,D^2,V$. 



The algorithm above provided the same performance as SoftImpute but with a much faster convergence.  Next, we incorporate Covariates into our calculations. We start by ignoring the regularization on the covariate coefficients.

The objective function in  6 becomes as follows:

$$
\begin{aligned}
minimize && \frac{1}{2} \|Y-AB^T - X\beta\|^2_F + \frac{\lambda}{2} (||A||^2_F+||B||^2_F)  && (ALS1)
\end{aligned}
$$
When $A$ is fixed, we estimate both $B$ and $\beta$. Similarly, when $B$ is fixed, we also get new estimates for $A$ and $\beta$.


*****************************************************************

**Algorithm ALS/Covariates**

1. Initialize $A = UD$ where $U_{n_1 \times r}$ is a randomly chosen matrix with orthonormal columns
   and $D = I_r$, the $r \times r$ identity matrix, and $B = VD$ with $V = 0$. Set $Y^*=Y^+=Y$. 
   Get initial estimates for $\hat{\beta} = (X^TX)^{-1}X^TY$.
   Alternatively, any prior solution $\beta$, $A = UD$ and $B = VD$ could be used as a warm start.

2. Repeat till convergence:

   a. Solve for $B$ and $\beta$ as:
   
      -  $DB^T = (D^2 + \lambda I)^{-1}D^2U^TY^+ = (D^2/(D^2+\lambda))U^TY^+$
      -  $\bar{B}D = \tilde{U}\tilde{D}^2\tilde{V}^T$
         -  $V \leftarrow \tilde{U}$
         -  $D^2 \leftarrow \tilde{D^2}$
      - $Y^*[\overline\Omega] = AB^T[\overline{\Omega}] + X \hat\beta[\overline{\Omega}] = UD^2V^T[\overline \Omega] + X \hat \beta[\overline{\Omega}]$
      - $\hat{\beta} = (X^TX)^{-1} X^T Y^*$
      -  $Y^+= Y^* - X \hat \beta$
   b. Solve for $A$ and $\beta$ as:
      -  $DA^T = (D^2 + \lambda I)^{-1}DV^T{Y^+}^T =  (D^2/(D^2+\lambda))(VY^+)^T$
      - $\bar{A}D = \tilde{U}\tilde{D}^2\tilde{V}^T$
         -  $U \leftarrow \tilde{U}$
         -  $D^2 \leftarrow \tilde{D^2}$
      - $Y^*[\overline\Omega] = AB^T[\overline{\Omega}] + X \hat\beta[\overline{\Omega}] = UD^2V^T[\overline \Omega] + X \hat \beta[\overline{\Omega}]$
      - $\hat{\beta} = (X^TX)^{-1} X^T Y^*$
      -  $Y^+ = Y^* - X \hat \beta$
   c. Compute Frobenious ratio to check for convergence $Frob(U_{old}, D^2_{old}, V_{old}, U, D^2, V)$


4. Compute the SVD decomposition  $Y^*V=U D_\sigma^2 V$
   - $D^2 = diag((\sigma_1-\lambda)_+,\cdots,(\sigma_r-\lambda)_+)$
   - $\hat{M} = U D^2 V^T$
   - $\hat{Y} = \hat{B} + X \beta$

**************************************************************

In simpler terms, we could write the algorithm above as:

********************************


**Algorithm ALS/Covariates**

1. Input: Data Matrix $Y$, Covariate Matrix $X$, initial $A,B,\beta$ and $k=0$
2. Output: $(A^*,B^*,\beta^*)$

Repeat Until Convergence:

   1. $k \leftarrow k+1$
   2. $Y^+ \leftarrow P_{\Omega}(Y)+P_{\overline \Omega}(AB^T)- X\beta =  P_{\Omega}(Y -AB^T) + (AB^T-X\beta)$
   3. $A \leftarrow Y^+ B(B^TB+\lambda I)^{-1}$
   4. $Y^* \leftarrow P_{\Omega}(Y)+P_{\overline \Omega}(AB^T+X\beta)$
   5. $\beta = (X^TX)^{-1}X^TY^*$
   6. $Y^+ \leftarrow Y^*- X\beta$
   7. $B^T \leftarrow (A^TA+\lambda I)^{-1}A^TY^+$

**************************************************************


## L2 Regularization on Covariate effects:

Consider the following model:



$$
\begin{aligned}
minimize && \frac{1}{2} \|Y- X\beta -AB^T \|^2_F + \lambda_1 \|\beta\|^2_F + \frac{\lambda_2}{2} (\|A\|^2_F+\|B\|^2_F)  && (ALS2)
\end{aligned}
$$

We attempted the following in order to find the values of the hyperparameters $\lambda_1$,  $\lambda_2$, and $r$ (the rank to be retained). Suppose that we have a list of possible of values for each parameter and that the optimal value is inside the list.

1. For each $\lambda_1$: fit a model (ALS1) to find the optimal $\lambda_2$ and $r$ and report the validation error. Pick $(\lambda_1^*,\lambda_2^*,r^*)$ corresponding to the minimum error. This method provides the best results, however, it's very slow as we multiply the time to fit a regular model by the number of $\lambda_1$ values. Assuming that we're separating the data into  training ($80\%$) and validation ($20\%$).

2. Changing the value of $\lambda_1$ within the model (ALS1) interferes with the convergence and is therefore avoided.

3. We noticed from method (1.) that the optimal value of $\lambda_2$ in (ALS2) is equal, almost all the times, to the optimal $\lambda_2$ from (ALS1), Therefore, I fitted the model (ALS1) first to obtain $\lambda_2^*$ and $r^*$($\lambda_1=0$). Then, I fitted model (ALS2) for all the values of $\lambda_1$ while setting $\lambda_2=\lambda_2^*$ and $r=r^*$. I used the same training-validation split method. The method improved the results and the fitting time was better than Mao and very close the training time in Model (ALS1). 

4. Finally, I did 3. while performing K-fold (K=3) cross-validation for computing the validation error (Average) and $r^*$ (also average). Note that $r$ is the number of non-zero Eigen values for a certain $\lambda_2$ choice, after being rounded to 6 decimals. 

--------------------

Below is illustration of Fitting Model (ALS1), (ALS2), and steps 1, 3 and 4 in the hyperparamter optimization.

We begin by simulation a 600x600 matrix with 10 covariates, collinearity, and $90%$ missing values.

```{r}
gen.dat <- generate_simulation_data_ysf(2,600,600,10,10, missing_prob = 0.9,coll=TRUE)
```

Split the observed portion into $80\%$ training and $20\%$ validation

```{r}
W_valid <- matrix.split.train.test(gen.dat$W, testp=0.2)
Y_train = (gen.dat$Y * W_valid)
Y_valid = gen.dat$Y[W_valid==0]
```

The following is the evaluation of $(X^TX)^{-1}X^T$ which is given to the soft-impute function. Could be modified to add $+ \lambda I_n$

```{r}
beta_partial = solve(t(gen.dat$X) %*% gen.dat$X) %*% t(gen.dat$X)
```


1. Illustration of fitting (ALS1) with a fixed $\lambda_2$ and $r$ (also referred to as $J$ or max-rank).

```{r}
lambda2 = 10
max.rank = 15
start_time <- Sys.time()
sout <- simpute.als.cov(Y_train, gen.dat$X, beta_partial,J = max.rank, thresh =  1e-3, lambda= lambda2,trace.it = TRUE,warm.start = NULL)
print(paste("Execution time is",round(as.numeric(difftime(Sys.time(), start_time,units = "secs"))), "seconds"))
sout$A_hat = sout$u %*% (sout$d * t(sout$v))
print(paste("Test error =", round(test_error(sout$A_hat[gen.dat$W==0], gen.dat$A[gen.dat$W==0]),5)))
```
Illustration of Model (ALS2) with fixed hyperparameters

```{r}
lambda1 = 10
beta_partial = solve(t(gen.dat$X) %*% gen.dat$X + diag(lambda1, ncol(gen.dat$X))) %*% t(gen.dat$X)

start_time <- Sys.time()
sout <- simpute.als.cov(Y_train, gen.dat$X, beta_partial,J = max.rank, thresh =  1e-3, lambda= lambda2,trace.it = TRUE,warm.start = NULL)
print(paste("Execution time is",round(as.numeric(difftime(Sys.time(), start_time,units = "secs"))), "seconds"))
sout$A_hat = sout$u %*% (sout$d * t(sout$v))
print(paste("Test error =", round(test_error(sout$A_hat[gen.dat$W==0], gen.dat$A[gen.dat$W==0]),5)))
```


2. Illustration of cross-validation (using validation set) for model (ALS1)

```{r}
start_time <- Sys.time()
sout <- simpute.cov.cv(Y_train, gen.dat$X, W_valid, Y_valid, 
                        trace=TRUE, rank.limit = 30, lambda1=0,n1n2 = 1, warm=NULL,tol = 2, print.best = FALSE)
print(paste("Execution time is",round(as.numeric(difftime(Sys.time(), start_time,units = "secs"))), "seconds"))
print(paste("Test error =", round(test_error(sout$A_hat[gen.dat$W==0], gen.dat$A[gen.dat$W==0]),5)))
print(sprintf("Optimal hyperparameters are: lambda2 = %.4f, r=%d", sout$lambda, sout$rank.max))
```

3. Illustration of cross-validation (attempt 1).

```{r}
start_time <- Sys.time()
lambda1.grid = seq(0,20, length.out=10)

sout <- softImputeALS_L2(Y_train, Y_valid, W_valid, gen.dat$X, lambda1.grid, no_cores=1, rank.limit=30) 
print(paste("Execution time is",round(as.numeric(difftime(Sys.time(), start_time,units = "secs"))), "seconds"))
print(paste("Test error =", round(test_error(sout$best_fit$A_hat[gen.dat$W==0], gen.dat$A[gen.dat$W==0]),5)))
print(sprintf("Optimal hyperparameters are: lambda1 = %.4f, lambda2 = %.4f, r=%d", sout$lambda1, sout$best_fit$lambda, sout$best_fit$rank.max))

```

4. Illustration of cross-validation (attempt 3)

```{r}
lambda1.grid = seq(0,30, length.out=10)

start_time <- Sys.time()
# 1. set lambda1=0 and find lambda2 and r
sout = simpute.cov.cv(Y_train, gen.dat$X, W_valid, Y_valid, trace=TRUE, rank.limit = 30, 
                             print.best=FALSE, rank.step=4, type="als", lambda1=0, tol=2)
# 2.  find lambda1 for the selected lambda2 and r
sout <- simpute.cov.cv.lambda1(Y_train, gen.dat$X, W_valid, Y_valid, sout$lambda, sout$rank.max, print.best = FALSE,
                                    trace=TRUE, lambda1.grid = lambda1.grid ,n1n2 = 1, warm=NULL)
print(paste("Execution time is",round(as.numeric(difftime(Sys.time(), start_time,units = "secs"))), "seconds"))
print(paste("Test error =", round(test_error(sout$A_hat[gen.dat$W==0], gen.dat$A[gen.dat$W==0]),5)))
print(sprintf("Optimal hyperparameters are: lambda1 = %.4f, lambda2 = %.4f, r=%d", sout$lambda1, sout$lambda2, sout$J))
   
```

5. Similar to 4 but with K-fold cross-validation

```{r}
lambda1.grid = seq(0,30, length.out=10)
n_folds = 5

start_time <- Sys.time()
# 1. set lambda1=0 and find lambda2 and r
sout = simpute.cov.kfold(gen.dat$Y, gen.dat$X, gen.dat$W, n_folds = n_folds, print.best = FALSE,
                                 trace=TRUE, rank.limit = 30, lambda1=0,n1n2 = 1, warm=NULL,tol = 2)
# 2.  find lambda1 for the selected lambda2 and r
sout <- simpute.cov.kfold.lambda1(gen.dat$Y, gen.dat$X, gen.dat$W, sout$lambda2, n_folds = n_folds, print.best = FALSE, 
                                         trace=TRUE,lambda1.grid = seq(0,20,length.out=20) ,n1n2 = 1, warm=NULL,
                                         J=c(sout$J))
print(paste("Execution time is",round(as.numeric(difftime(Sys.time(), start_time,units = "secs"))), "seconds"))
print(paste("Test error =", round(test_error(sout$A_hat[gen.dat$W==0], gen.dat$A[gen.dat$W==0]),5)))
print(sprintf("Optimal hyperparameters are: lambda1 = %.4f, lambda2 = %.4f, r=%d", sout$lambda1, sout$lambda2, sout$J))

```


$$
\begin{aligned}
\end{aligned}
$$

