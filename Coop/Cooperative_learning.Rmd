---
title: "Cooperative Learning"
author: "Khaled Fouda"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---


```{r setup, include=TRUE}
path_to_code <- "/mnt/campus/math/research/kfouda/main/HEC/Youssef/HEC_MAO_COOP/"
knitr::opts_chunk$set(echo = TRUE, eval = FALSE,  cache = TRUE)
knitr::opts_knit$set(root.dir=path_to_code)
```


```{r , include=TRUE, eval=TRUE, message=FALSE,warning=FALSE}
path_to_code = "./code_files/"
library(knitr)
library(kableExtra)
library(tidyverse)
library(magrittr)
require(foreach)
require(doParallel)
source(paste0(path_to_code,"Mao_import_lib.R"))
```


### Model definition and Notations

Let the response vector $Y \in \Re^n$ and the covariate matrices $X \in \Re^{n\times p_x}$ and $Z \in \Re^{n \times p_z}$. The following quanitity is proposed for minimization

$$
\begin{aligned}
min E[\frac{1}{2}(y - f_x(X)-f_z(Z))^2+\frac{\rho}{2}(f_x(X)-f_z(Z))^2]&&(1)
\end{aligned}
$$
where the second term is the agreement penalty, encouraging the predictions from different views to agree. Equation 1 has fixed points:

$$
\begin{aligned}
f_x(X) &= E[\frac{1}{1+\rho} (y+(1-\rho)f_z(Z))|X] &&\\
f_z(Z) &= E[\frac{1}{1+\rho} (y+(1-\rho)f_x(X))|Z] && (2)
\end{aligned}
$$
Where the objective can be optimized by repeatedly updating the fir for each covariate matrix in turn, holding the other matrix fixed.

#### Case: Regularized Linear Regression

Here, we assume that the columns of $X$ and $Z$ are standardized and y has mean zero. Hence, we can omit the intercept. The resulting objective function is:


$$
\begin{aligned}
J(\theta_x,\theta_z) = \frac{1}{2} || y - X \theta_x - Z \theta_z ||^2 + \frac{\rho}{2} ||X\theta_x-Z\theta_z||^2+\lambda_xP^x(\theta_x)+\lambda_zP^z(\theta_z) && (3)
\end{aligned}
$$

Where $P^x(\theta_x)$ can be any regularization term. Let's assume $L_1$ regularization in $\theta_x$ and $\theta_z$. Moreover, the author showed that generally, there is no advantage to allowing $\lambda_x$ and $\lambda_z$ be different. The only case where it was appropriate to make them different is when one of the covariate matrices doen't contribute at all to the model. We will assume them equal for now, as did the author.

$$
\begin{aligned}
J(\theta_x,\theta_z) = \frac{1}{2} || y - X \theta_x - Z \theta_z ||^2 + \frac{\rho}{2} ||X\theta_x-Z\theta_z||^2+\lambda (||\theta_x||+||\theta_z||) && (5)
\end{aligned}
$$

We could rewrite equation 5 as:

$$
\begin{aligned}
J(\theta_x,\theta_z) = \frac{1}{2} || \tilde y - \tilde{X}\tilde\beta ||^2 +\lambda (||\theta_x||+||\theta_z||) && (7)
\end{aligned}
$$
where


$$
\begin{aligned}
\tilde X = \matrix{X&Z\\-\sqrt \rho X&\sqrt \rho X} && \tilde y = \matrix{y\\0} && \tilde \beta = \matrix{\theta_x\\\theta_z}&& (6)
\end{aligned}
$$
Equation 5 is a form of lasso which then we could use the glmnet package to git the model. 
Let the $Lasso(X,y,\lambda)$ denote the generic problem:

$$
\begin{aligned}
min_\beta \frac{1}{2} || y - X\beta||^2 + \lambda || \beta|| && (8)
\end{aligned}
$$
We could also incorporate $L_2$ penalties to the objective in 5. This option is included in their software implementation.


$$
\begin{aligned}
\lambda[ (1-\alpha)(||\theta_x||+||\theta_z||)+\alpha(||\theta_x||^2_2/2+||\theta_z||^2_2/2) ] && (9)
\end{aligned}
$$

### Solution

The author provided two alogrithms to solve equation 5.

#### Algorithm 1: Direct Calculation


for each $\rho$ in the grid, we solve  $Lasso(\tilde X,\tilde y,\lambda)$ over a decreasing grid of $\lambda$.

#### Algorithm 2: one-at-a-time for regularized linear regression

the updates are (for each $\lambda_x$, $\lambda_z$, and $\rho$ in the grid):

$$
\begin{aligned}
\hat\theta_x &= Lasso(X,y_x^*,\lambda_x) && where && y_x^*=\frac{y}{1+p}-\frac{(1-p)Z\theta_z}{1+p},\\ 
\hat\theta_z &= Lasso(X,y_z^*,\lambda_z) && where && y_z^*=\frac{y}{1+p}-\frac{(1-p)X\theta_x}{1+p}.
\end{aligned} 
$$

## Code and R package

The simulation and implementation code can be found at

https://github.com/dingdaisy/cooperative-learning

and the R package Multiview at https://cran.r-project.org/web/packages/multiview/. The package includes functions to apply cooperative learning on two or more views using linear, poisson, and logistic regression. The package uses glmnet. The code for the functions can also be found in the github.

----------------------------
---------------------------

# Mao-Coop Model


The following model is introduced by Youssef in page 4 of his notes.

\begin{equation}
    A = (XA + \Theta_X) + (ZB + \Theta_Z)^T,\\
    Y = (A+E) * W
\end{equation}


with the following loss function

$$
\begin{align}
    \text{Loss-3} =& \left\|  Y - (XA + \Theta_X) - (ZB + \Theta_Z)^T \right\|_F^2 +\\& \rho \left\| (XA + \Theta_X) - (ZB + \Theta_Z)^T \right\|_F^2 +\\
    & \lambda_1 \left\| A \right\|_F^2 + \lambda_2 \left( (1 - \alpha_1) \left\| \Theta_X \right\|_F^2 + \alpha_1 \left\| \Theta_X \right\|_* \right)+\\
    & \lambda_3 \left\| B \right\|_F^2 + \lambda_4 \left( (1 - \alpha_2) \left\| \Theta_Z \right\|_F^2 + \alpha_2 \left\| \Theta_Z \right\|_* \right).&&(Y0)
\end{align}
$$


with $\Theta_X$ and $\Theta_Z$ low rank matrices that we hope it will capture the part of $Y$ that is not linearly explained by either $X$ or $Z$. Furthermore, assume that the column spaces of $X$ and $\Theta_X$ are orthogonal, and that the column spaces of $Z$ and $\Theta_Z$ are orthogonal. 

To get the quantities of interest, a possible approach is to solve in an iterative way the following

$$
\begin{aligned}
\hat{A}, \hat{\Theta}_X =& \arg \min_{A,\Theta_X} \left\| \frac{1}{1 + \rho}  Y - \frac{1 - \rho}{1 + \rho} (ZB + \Theta_Z)^T - (XA + \Theta_X) \right\|_F^2 + \\
 &\lambda_1 \left\| A \right\|_F^2 + \lambda_2 \left((1 - \alpha_1) \left\| \Theta_X \right\|_F^2 + \alpha_1 \left\| \Theta_X \right\|_* \right), && (Y1) \\
\hat{Z}, \hat{\Theta}_Z =& \arg \min_{B,\Theta_Z} \left\| \frac{1}{1 + \rho} Y^T - \frac{1 - \rho}{1 + \rho} (XA + \Theta_X)^T - (ZB + \Theta_Z) \right\|_F^2 +\\
 &\lambda_3 \left\| B \right\|_F^2 + \lambda_4 \left((1 - \alpha_2) \left\| \Theta_Z \right\|_F^2 + \alpha_2 \left\| \Theta_Z \right\|_* \right). && (Y2)
\end{aligned}
$$

### Interpretation of the solution

The the first two terms in the loss in (Y0) has the same form as in Mao's formula and hence if we replace these terms with following two terms

$$
\| f_x(X) - \frac{y}{1+\rho} - \frac{1-\rho}{1+\rho}f_z(Z)\|^2 \\
\| f_z(Z) - \frac{y}{1+\rho} - \frac{1-\rho}{1+\rho}f_x(x)\|^2
$$
where in our case $f_x(X)=XA+\Theta_X$ and $f_z(Z)=(ZB+\Theta_Z)^T$. 

### Connection to Mao

In order to iteratively solve (Y1) and (Y2) using an a method similar to the one-at-a-time algorithm, we will apply Mao method to minimize each formula. First, we need to have them in the form of equations (4) and (5) from Mao's paper (check the other html file).

The regularization terms match exactly the form and hence we don't need to adjust them. 
For the first term in (Y1), let 
$$Y^* = \frac{\ Y}{1+\rho} - \frac{1-\rho}{1+\rho}\hat {f_z}(Z) = \frac{Y}{1+\rho} - \frac{1-\rho}{1+\rho}(Z\hat{B}+\hat{\Theta}_Z)^T$$
where $\hat {f_z}(Z) = Z\hat{B}+\hat{\Theta}_Z$ are the estimates from fitting (Y2) in the previous iteration.  

Similarly, for the first term in (Y2), let

$$Y^* = \frac{\hat Y}{1+\rho} - \frac{1-\rho}{1+\rho}\hat {f_x}(x) = \frac{Y}{1+\rho} - \frac{1-\rho}{1+\rho}(X\hat{A}+\hat{\Theta}_X)$$
where $\hat {f_x}(X) = X\hat{A}+\hat{\Theta}_X$ are the estimates from fitting (Y1) in the previous iteration.


### Implementation 

#### Note:

In Youssef's implementation, a $\rho=1$ was assumed. That means  that we are fitting two different models and taking the average of the estimation of each. In that case, there's no benefit from applying the Mao-Coop model and it would be equivalent to fit two different Mao models and take the final average. 



```{r code=readLines(paste0(path_to_code, "/Mao_Coop_fit.R")),eval=TRUE}

```


```{r eval=TRUE}

gen.dat <- generate_simulation_data_ysf(1,200,200,5,5, missing_prob =0.9 ,coll=TRUE)

A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,10,1e-3,1,alpha_grid = c(1))
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,20,1e-3,0.7,alpha_grid = c(1))
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,20,1e-3,0.9,alpha_grid = c(1))
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,10,1e-3,0.5,alpha_grid = c(1))
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,4,1e-3,0.3,alpha_grid = c(1))
A.hat <- Mao_Coop_fit(gen.dat$A, gen.dat$X, gen.dat$Z, gen.dat$W,4,1e-3,0.01,alpha_grid = c(1))

```


I slightly modified Youssef's implementation so that the simulation parameters, error function, and the seed are equivalent. I ran his Mao_coop.R file and copied the output below.
```

[1] "-----------------------------------------------------------------"
[1] 1
[1] "Agreement Penalty = 1"
[1] "MSE X=0.856599876541136, MSE Z=0.858272657508831, MSE Avg=0.858272657508831"
[1] "iter=1, diff=0.173776702075844, error=0.796066844957261"
[1] "iter=2, diff=0.356368431975322, error=0.782690155609017"
[1] "iter=3, diff=0.28812858324541, error=0.797803826529267"
[1] "iter=4, diff=0.155496291739101, error=0.784199848873508"
[1] "iter=5, diff=0.161892463998242, error=0.793393444968743"
[1] "iter=6, diff=0.317057152329823, error=0.801405577581984"
[1] "iter=7, diff=0.30676328528986, error=0.786143081889125"
[1] "iter=8, diff=0.253978797736027, error=0.791680055210597"
[1] "iter=9, diff=0.232163178179326, error=0.787114557949007"
[1] "iter=10, diff=0.195383853747667, error=0.777777308274588"
[1] "Run time is 7.306 minutes."
[1] "Average Run time per iteration is 0.7306 minutes."
[1] "Agreement Penalty = 0.9"
[1] "MSE X=0.856599876541136, MSE Z=0.858272657508831, MSE Avg=0.858272657508831"
[1] "iter=1, diff=0.169268570227897, error=0.803307247628354"
[1] "iter=2, diff=0.469387820969411, error=0.773461387834371"
[1] "iter=3, diff=0.304893512098015, error=0.792090767992488"
[1] "iter=4, diff=0.159194959780592, error=0.778001412320058"
[1] "iter=5, diff=0.162989493266899, error=0.787199603208735"
[1] "iter=6, diff=0.330084977654404, error=0.795134019694121"
[1] "iter=7, diff=0.319300139244977, error=0.778424221211666"
[1] "iter=8, diff=0.305190031566121, error=0.785899328679497"
[1] "iter=9, diff=0.256189653312628, error=0.781221339556741"
[1] "iter=10, diff=0.216635183327255, error=0.774563252949486"
[1] "Run time is 7.113 minutes."
[1] "Average Run time per iteration is 0.7113 minutes."
[1] "Agreement Penalty = 0.5"
[1] "MSE X=0.856599876541136, MSE Z=0.858272657508831, MSE Avg=0.858272657508831"
[1] "iter=1, diff=0.437579418283936, error=0.771149085297839"
[1] "iter=2, diff=0.388586744717726, error=0.758194105488018"
[1] "iter=3, diff=0.364672338096423, error=0.770082761328291"
[1] "iter=4, diff=0.705630200404503, error=0.792106327352782"
[1] "iter=5, diff=3.69816718360667, error=1.74572414293533"
[1] "iter=6, diff=17.1979821784853, error=40.6609818807247"
[1] "iter=7, diff=74.7561662437278, error=851.364738190654"
[1] "iter=8, diff=1019.60118720525, error=121248.288585989"
[1] "iter=9, diff=10001.0894671484, error=18772692.368711"
[1] "iter=10, diff=15864.3035230347, error=78614411.1213746"
[1] "Run time is 4.903 minutes."
[1] "Average Run time per iteration is 0.4903 minutes."
[1] "Agreement Penalty = 0.01"
[1] "MSE X=0.856599876541136, MSE Z=0.858272657508831, MSE Avg=0.858272657508831"
[1] "iter=1, diff=26.5885395767465, error=0.833846795289895"
[1] "iter=2, diff=4782.22036581764, error=5963.34286409363"
[1] "iter=3, diff=1434669.8962026, error=275865896.459691"
[1] "iter=4, diff=317693232.190757, error=17168861336918"
[1] "iter=5, diff=71429002792.437, error=867801519648958976"
[1] "iter=6, diff=16058833633188.7, error=4.38630909079698e+22"
[1] "iter=7, diff=3610384120048023, error=2.21706310529362e+27"
[1] "iter=8, diff=811694908362535168, error=1.12061615133616e+32"
[1] "iter=9, diff=1.82487126675294e+20, error=5.66416244819358e+36"
[1] "iter=10, diff=4.78221397385797e+22, error=2.80225794726522e+41"
[1] "Run time is 4.977 minutes."
[1] "Average Run time per iteration is 0.4977 minutes."
[1] "end ----------------"

```


$$
\begin{aligned}
\end{aligned}
$$